{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install SDG\n",
    " - git clone https://github.com/Red-Hat-AI-Innovation-Team/SDG-Research.git && cd SDG-Research\n",
    " - pip install -r requirements.txt\n",
    " - pip install -e .\n",
    " - pip install rich datasets tabulate transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Party\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "import click\n",
    "\n",
    "# First Party\n",
    "from instructlab.sdg.flow import Flow\n",
    "from instructlab.sdg.pipeline import Pipeline\n",
    "from instructlab.sdg.sdg import SDG\n",
    "from instructlab.sdg.utils.docprocessor import DocProcessor\n",
    "from utils.data import postprocess_and_save, pretty_print_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Seed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not a conda environment: /workspace/home/lab/.conda/envs/docling\n",
      "\n",
      "EnvironmentLocationNotFound: Not a conda environment: /workspace/home/lab/.conda/envs/docling\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!OMP_NUM_THREADS=32 mamba run -n docling python /workspace/home/lab/abhi/sdg_demo/SDG-Research/scripts/docparser.py --input-dir {data_dir} --output-dir {data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"sdg_demo_output/\"\n",
    "# This is where your PDFs are stored\n",
    "data_dir = 'document_collection/RBC' \n",
    "# It also have your QNA yaml file\n",
    "dp = DocProcessor(data_dir, user_config_path=f'{data_dir}/qna.yaml')\n",
    "seed_data = dp.get_processed_dataset()\n",
    "seed_data.to_json(f'{output_dir}/seed_data.jsonl', orient='records', lines=True)\n",
    "pretty_print_dict(f'{output_dir}/seed_data.jsonl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup OpenAI Client for interacting with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[18:27:47] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">GET</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:8000/v1/models</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>             <a href=\"file:///home/lab/.conda/envs/sdg_research/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/lab/.conda/envs/sdg_research/lib/python3.11/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[18:27:47]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mGET\u001b[0m \u001b[4;94mhttp://localhost:8000/v1/models\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m             \u001b]8;id=806909;file:///home/lab/.conda/envs/sdg_research/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=355886;file:///home/lab/.conda/envs/sdg_research/lib/python3.11/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai/Mixtral-8x7B-Instruct-v0.1\n"
     ]
    }
   ],
   "source": [
    "endpoint = f\"http://localhost:8000/v1\"\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = endpoint\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "teacher_model = client.models.list().data[0].id\n",
    "print(teacher_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run SDG\n",
    "- This will create knowledge flow from provided yaml file\n",
    "- We will run this on small dataset for demo purposes\n",
    "- For large scale generation, please use the python command provided in the next cell\n",
    "- You can analyze the generated data to ensure the quality is similar to proivded QnA pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_agentic_pipeline = \"/workspace/home/lab/abhi/sdg_demo/SDG-Research/src/instructlab/sdg/flows/generation/knowledge/synth_knowledge1.5.yaml\"\n",
    "flow_cfg = Flow(client).get_flow_from_file(knowledge_agentic_pipeline)\n",
    "sdg = SDG(\n",
    "    [Pipeline(flow_cfg)],\n",
    "    num_workers=1,\n",
    "    batch_size=1,\n",
    "    save_freq=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_samples = 5\n",
    "ds = load_dataset('json', data_files=f'{output_dir}/seed_data.jsonl', split='train')\n",
    "ds = ds.shuffle(seed=42).select(range(number_of_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint directory is used to save the intermediate datasets\n",
    "generated_data = sdg.generate(ds, checkpoint_dir=\"Tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run SDG through python command (For large scale generation)\n",
    "\n",
    "```python\n",
    "python /home/lab/sdg/scripts/generate.py --ds_path {output_dir}/seed_data.jsonl --bs 8 --num_workers 8 --save_path {output_dir}/gen.jsonl --flow SynthKnowledgeFlow1.5 --endpoint {teacher_endpoint_url} --checkpoint_dir {output_dir}/data_checkpoints --save_freq 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the generated data into training format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the system prompt for RHELAI 1.4.1\n",
    "system_prompt_rhelai_1_4_1 = (\n",
    "    \"I am a Red HatÂ® Instruct Model, an AI language model developed by Red Hat and IBM Research based on the granite-3.1-8b-base model. My primary role is to serve as a chat assistant.\"\n",
    ")\n",
    "precomputed_skills_path = \"<RHELAI precomputed skills path>\"\n",
    "# Download the RHELAI 1.4.1 data here: https://drive.google.com/file/d/1q8Rxcat5dZxXP-LqgPSCUsyttyAn6aLJ/view?usp=sharing\n",
    "# Unzip the folder and put the path to skills.jsonl in precomputed_skills_path\n",
    "postprocess_and_save(f\"{output_dir}/gen.jsonl\", dataset_save_path=f'{output_dir}', precomputed_skills_path=precomputed_skills_path, sys_prompt=system_prompt_rhelai_1_4_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdg_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

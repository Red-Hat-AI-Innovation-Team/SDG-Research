{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.conda/envs/sdg_research_upstream/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-20 18:20:58 [__init__.py:256] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 18:20:59,120\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from vllm import LLM\n",
    "from vllm import LLM, SamplingParams\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"nvidia/Llama-3_3-Nemotron-Super-49B-v1\") # microsoft/phi-4\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system<|im_sep|>detailed thinking on<|im_end|><|im_start|>user<|im_sep|>Hello, how are you?<|im_end|><|im_start|>assistant<|im_sep|>\n"
     ]
    }
   ],
   "source": [
    "thinking = \"on\"\n",
    "print(tokenizer.apply_chat_template([{\"role\": \"system\", \"content\": f\"detailed thinking {thinking}\"}, {'role': 'user', 'content': 'Hello, how are you?'}], add_generation_prompt=True, tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "            model= \"nvidia/Llama-3_3-Nemotron-Super-49B-v1\", #\"/new_data/experiments_rh/ab-bmo-phi-4-mini-instruct/lr_5e-06_bs_128_bmo_lab_stack_w_doc_outline/hf_format/samples_32218\",\n",
    "            gpu_memory_utilization=0.8,\n",
    "            enable_prefix_caching=False,\n",
    "            seed=42,\n",
    "            tensor_parallel_size=8,\n",
    "            trust_remote_code=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% for message in messages %}{% if (message['role'] == 'system') %}{{'<|im_start|>system<|im_sep|>' + message['content'] + '<|im_end|>'}}{% elif (message['role'] == 'user') %}{{'<|im_start|>user<|im_sep|>' + message['content'] + '<|im_end|>'}}{% elif (message['role'] == 'assistant') %}{{'<|im_start|>assistant<|im_sep|>' + message['content'] + '<|im_end|>'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant<|im_sep|>' }}{% endif %}\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Teacher Model\n",
    "```shell\n",
    "export HUGGINGFACE_HUB_CACHE=\"/new_data/hf_cache\"   \n",
    "export HF_DATASETS_CACHE=\"/dev/shm/hf\"\n",
    "export HF_HOME=\"/new_data/hf_cache\"\n",
    "export HF_MODEL_CACHE=\"/new_data/hf_cache\"\n",
    "\n",
    "port=8000\n",
    "for i in {0..7}; do\n",
    "    CUDA_VISIBLE_DEVICES=$i python -m vllm.entrypoints.openai.api_server \\\n",
    "        --model microsoft/phi-4 \\\n",
    "        --dtype float16 \\\n",
    "        --tensor-parallel-size 1 \\\n",
    "        --port $port \\\n",
    "        --trust-remote-code > run_phi4_teacher_model_$i.log 2>&1 &\n",
    "    port=$((port + 1))\n",
    "done\n",
    "```\n",
    "\n",
    "### Run SDG in parallel\n",
    "```shell\n",
    "# Get dataset size and save into variable\n",
    "dataset_size=$(wc -l /new_data/knowledge_rh/quality/base_datasets/quality_seed_example_chunked.jsonl | awk '{print $1}')\n",
    "number_of_processes=8\n",
    "port=8000\n",
    "for i in {0..7}; do\n",
    "    dataset_start_index=$((i * dataset_size / number_of_processes))\n",
    "    dataset_end_index=$((dataset_start_index + dataset_size / number_of_processes))\n",
    "    python scripts/generate.py --ds_path  /new_data/knowledge_rh/quality/base_datasets/quality_seed_example_chunked.jsonl \\\n",
    "        --bs 2 --num_workers 30 \\\n",
    "        --save_path /new_data/knowledge_rh/quality/knowledge1.25_extractive_phi4/gen.jsonl \\\n",
    "        --flow /home/lab/abhi/SDG-Research-Upstream/src/instructlab/sdg/flows/generation/knowledge/synth_knowledge1.5_phi4.yaml \\\n",
    "        --endpoint http://localhost:$port/v1 \\\n",
    "        --checkpoint_dir /new_data/knowledge_rh/quality/knowledge1.25_extractive_phi4/data_checkpoints \\\n",
    "        --save_freq 1000 \\\n",
    "        --dataset_start_index $dataset_start_index \\\n",
    "        --dataset_end_index $dataset_end_index > run_sdg_$i.log 2>&1 &\n",
    "    echo \"Starting process $i with dataset from $dataset_start_index to $dataset_end_index on port $port\"\n",
    "    port=$((port + 1))\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
